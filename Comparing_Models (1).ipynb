{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Comparing-Models.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "GOnesgCrEzbS",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
        "\n",
        "<h1 align=center><font size = 5>Peer Review Final Assignment</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "HiCGzWoAEzbX",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "t3_YnkgyEzba",
        "colab_type": "text"
      },
      "source": [
        "In this lab, you will build an image classifier using the VGG16 pre-trained model, and you will evaluate it and compare its performance to the model we built in the last module using the ResNet50 pre-trained model. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "Odd0qyGBEzbc",
        "colab_type": "text"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>    \n",
        "\n",
        "1. <a href=\"#item41\">Download Data \n",
        "2. <a href=\"#item42\">Part 1</a>\n",
        "3. <a href=\"#item43\">Part 2</a>  \n",
        "4. <a href=\"#item44\">Part 3</a>  \n",
        "\n",
        "</font>\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "M5ZHMt0xEzbc",
        "colab_type": "text"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZldVafpqEzbd",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"item41\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPgEpM3Ezbe",
        "colab_type": "text"
      },
      "source": [
        "<h1>---------------------------------------------------Resnet50 Model --------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfhsaIpfsCnL",
        "colab_type": "text"
      },
      "source": [
        "### These Steps can be skipped and by using load model function we can load the previously trained model and then can evaluate it on dataset of week4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJB2uha3Ezbg",
        "colab_type": "text"
      },
      "source": [
        "<h2>Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4PE8ZfcEzbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "d8d59135-9be9-46ba-db72-de3114642141"
      },
      "source": [
        "## get the data\n",
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-10 21:44:02--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261482368 (249M) [application/zip]\n",
            "Saving to: ‘concrete_data_week3.zip.3’\n",
            "\n",
            "concrete_data_week3 100%[===================>] 249.37M  30.2MB/s    in 8.2s    \n",
            "\n",
            "2020-07-10 21:44:11 (30.4 MB/s) - ‘concrete_data_week3.zip.3’ saved [261482368/261482368]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUMZOHJPEzbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip concrete_data_week3.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXZHXd7Ezb0",
        "colab_type": "text"
      },
      "source": [
        "<h2>Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSu5WSsiEzb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "589c024e-01a7-44d8-9222-638832cb1645"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.applications import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "print(keras.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wswiiSpWEzb9",
        "colab_type": "text"
      },
      "source": [
        "<h2>Define Global Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXtKTfufEzb-",
        "colab_type": "text"
      },
      "source": [
        "Here, we will define constants that we will be using throughout the rest of the lab. \n",
        "\n",
        "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
        "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
        "3. We will training and validating the model using batches of 100 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE8UN4LIEzb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 2\n",
        "\n",
        "image_resize = 224\n",
        "\n",
        "batch_size_training = 100\n",
        "batch_size_validation = 100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I4gvn4cEzcE",
        "colab_type": "text"
      },
      "source": [
        "## Construct ImageDataGenerator Instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHTQdOr2EzcE",
        "colab_type": "text"
      },
      "source": [
        "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzkGm3HaEzcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY2Yd1EeEzcU",
        "colab_type": "text"
      },
      "source": [
        "Next, we will use the *flow_from_directory* method to get the training images as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHHsvfPgEzcW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "818ca0cd-c405-4319-d003-074268039796"
      },
      "source": [
        "train_generator = data_generator.flow_from_directory(\n",
        "    'concrete_data_week3/train',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 30001 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14TDcjOVEzce",
        "colab_type": "text"
      },
      "source": [
        "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y04cYYtWEzcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90e44c42-9a58-4fb5-c024-9de6b42a53e2"
      },
      "source": [
        "## Type your answer here\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    'concrete_data_week3/valid',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_validation,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10001 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW_4eYgBEzck",
        "colab_type": "text"
      },
      "source": [
        "## Build, Compile and Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAp2PFeqEzcl",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will start building our model. We will use the Sequential model class from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrgOOhbGEzcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "#model = tensorflow.keras.Sequential()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UEiLaizEzcw",
        "colab_type": "text"
      },
      "source": [
        "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekD_q_ECEzcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(ResNet50(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    ))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtviwS7UEzc5",
        "colab_type": "text"
      },
      "source": [
        "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0FLMGNDEzc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5in_OnEzdC",
        "colab_type": "text"
      },
      "source": [
        "You can access the model's layers using the *layers* attribute of our model object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBgblxsKEzdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f461ed2e-40f0-4bc7-969d-b920fcff57d7"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.training.Model at 0x7f4340176898>,\n",
              " <keras.layers.core.Dense at 0x7f439dca6780>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9A7HuqvEzdM",
        "colab_type": "text"
      },
      "source": [
        "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg4pEjrLEzdN",
        "colab_type": "text"
      },
      "source": [
        "You can access the ResNet50 layers by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z557620EzdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd3aa720-a692-4ecb-8d7a-69678047e75d"
      },
      "source": [
        "model.layers[0].layers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f439dca65c0>,\n",
              " <keras.layers.convolutional.ZeroPadding2D at 0x7f439dca6828>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439dca6a58>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439dca69b0>,\n",
              " <keras.layers.core.Activation at 0x7f439dca6b70>,\n",
              " <keras.layers.convolutional.ZeroPadding2D at 0x7f43fc567e48>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f439cc23160>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439cc23f28>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c7866d8>,\n",
              " <keras.layers.core.Activation at 0x7f439c7589e8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c7588d0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c764780>,\n",
              " <keras.layers.core.Activation at 0x7f439c6f4be0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c70cdd8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c7262b0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c726898>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c6bfcf8>,\n",
              " <keras.layers.merge.Add at 0x7f439c6cec88>,\n",
              " <keras.layers.core.Activation at 0x7f439c6e82b0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c6e0e80>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c68acc0>,\n",
              " <keras.layers.core.Activation at 0x7f439c6937b8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c6a25f8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c6368d0>,\n",
              " <keras.layers.core.Activation at 0x7f439c63ca58>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c653e80>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c66a748>,\n",
              " <keras.layers.merge.Add at 0x7f439c66a160>,\n",
              " <keras.layers.core.Activation at 0x7f439c5fc908>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c5fc9e8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c61e278>,\n",
              " <keras.layers.core.Activation at 0x7f439c5af9b0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c5b9160>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c5cea20>,\n",
              " <keras.layers.core.Activation at 0x7f439c5ce438>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c5ed780>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c573240>,\n",
              " <keras.layers.merge.Add at 0x7f439c585d30>,\n",
              " <keras.layers.core.Activation at 0x7f439c59cfd0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c5a2518>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c537e10>,\n",
              " <keras.layers.core.Activation at 0x7f439c53eb70>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c54ed68>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c553518>,\n",
              " <keras.layers.core.Activation at 0x7f439c567e80>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c5032e8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c519f60>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c5147f0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c4ba5f8>,\n",
              " <keras.layers.merge.Add at 0x7f439c4ca5c0>,\n",
              " <keras.layers.core.Activation at 0x7f439c4d6710>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c4d6b70>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c1c7da0>,\n",
              " <keras.layers.core.Activation at 0x7f439c1cfda0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c1e3f28>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c1f77b8>,\n",
              " <keras.layers.core.Activation at 0x7f439c1f71d0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c197518>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c1a9ef0>,\n",
              " <keras.layers.merge.Add at 0x7f439c1afac8>,\n",
              " <keras.layers.core.Activation at 0x7f439c144d68>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c144e80>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c164ba8>,\n",
              " <keras.layers.core.Activation at 0x7f439c169908>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c17aa20>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c1022b0>,\n",
              " <keras.layers.core.Activation at 0x7f439c112da0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c132198>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c0c1b70>,\n",
              " <keras.layers.merge.Add at 0x7f439c0c1588>,\n",
              " <keras.layers.core.Activation at 0x7f439c0ded30>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c0dee10>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c07ec50>,\n",
              " <keras.layers.core.Activation at 0x7f439c085710>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c093588>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c0a6860>,\n",
              " <keras.layers.core.Activation at 0x7f439c0acfd0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c042ba8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c0596d8>,\n",
              " <keras.layers.merge.Add at 0x7f439c059208>,\n",
              " <keras.layers.core.Activation at 0x7f439c076898>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c076978>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439c00e208>,\n",
              " <keras.layers.core.Activation at 0x7f439c01f0b8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439c02d0f0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bfbe9b0>,\n",
              " <keras.layers.core.Activation at 0x7f439bfbe3c8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bfd8710>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bff1cc0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bfe21d0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bf8fa20>,\n",
              " <keras.layers.merge.Add at 0x7f439bfa29e8>,\n",
              " <keras.layers.core.Activation at 0x7f439bfaef98>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bfaeba8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bf5d9e8>,\n",
              " <keras.layers.core.Activation at 0x7f439bf63278>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bf73320>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bf05be0>,\n",
              " <keras.layers.core.Activation at 0x7f439bf055f8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bf20c50>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bf27400>,\n",
              " <keras.layers.merge.Add at 0x7f439bf3ad68>,\n",
              " <keras.layers.core.Activation at 0x7f439bed6630>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bed6710>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439beeed30>,\n",
              " <keras.layers.core.Activation at 0x7f439bef7d30>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439be84eb8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439be9d748>,\n",
              " <keras.layers.core.Activation at 0x7f439be9d160>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439beba4a8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439be4eeb8>,\n",
              " <keras.layers.merge.Add at 0x7f439be53a58>,\n",
              " <keras.layers.core.Activation at 0x7f439be6bcf8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439be6bfd0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439be09be0>,\n",
              " <keras.layers.core.Activation at 0x7f439be0e898>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439be1d9b0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439be24240>,\n",
              " <keras.layers.core.Activation at 0x7f439be35d30>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bdcdf98>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bde7b00>,\n",
              " <keras.layers.merge.Add at 0x7f439bde7518>,\n",
              " <keras.layers.core.Activation at 0x7f439bd83cc0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bd83da0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bda1be0>,\n",
              " <keras.layers.core.Activation at 0x7f439bda94e0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bdb7518>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bd497f0>,\n",
              " <keras.layers.core.Activation at 0x7f439bd4ef60>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bd65ef0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bcfe668>,\n",
              " <keras.layers.merge.Add at 0x7f439bcfef60>,\n",
              " <keras.layers.core.Activation at 0x7f439bd1c828>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bd1c908>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bd34198>,\n",
              " <keras.layers.core.Activation at 0x7f439bd39f60>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bcd2080>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bce4940>,\n",
              " <keras.layers.core.Activation at 0x7f439bce4358>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bcf96a0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439bc85278>,\n",
              " <keras.layers.merge.Add at 0x7f439bc99c50>,\n",
              " <keras.layers.core.Activation at 0x7f439bcaef98>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f439bcaef60>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439006cd30>,\n",
              " <keras.layers.core.Activation at 0x7f4390073a90>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4390083c88>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439008a438>,\n",
              " <keras.layers.core.Activation at 0x7f439009bda0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f43900382b0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4390051fd0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f439004bd68>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f4340296630>,\n",
              " <keras.layers.merge.Add at 0x7f43402a8588>,\n",
              " <keras.layers.core.Activation at 0x7f43402b3e10>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f43402b3780>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f434025bf60>,\n",
              " <keras.layers.core.Activation at 0x7f4340261cc0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4340271dd8>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f434020b6d8>,\n",
              " <keras.layers.core.Activation at 0x7f434020b208>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4340227438>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f4340239e10>,\n",
              " <keras.layers.merge.Add at 0x7f434023fac8>,\n",
              " <keras.layers.core.Activation at 0x7f43401d6c88>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f43401d6fd0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f43401f4b70>,\n",
              " <keras.layers.core.Activation at 0x7f43401fab00>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4340189940>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f43401911d0>,\n",
              " <keras.layers.core.Activation at 0x7f43401a4cc0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f43401bcef0>,\n",
              " <keras.layers.normalization.BatchNormalization at 0x7f4340152a90>,\n",
              " <keras.layers.merge.Add at 0x7f43401524a8>,\n",
              " <keras.layers.core.Activation at 0x7f4340170c50>,\n",
              " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f4340170d30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1DfYy-rEzdW",
        "colab_type": "text"
      },
      "source": [
        "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZxn34-dEzdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[0].trainable = False"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytT0BL2REzdg",
        "colab_type": "text"
      },
      "source": [
        "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ-5npCMEzdg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "1eb07ed8-4a07-4bb2-d073-7482ab5d8207"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Model)             (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxUuJ1OTEzdj",
        "colab_type": "text"
      },
      "source": [
        "Next we compile our model using the **adam** optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOafB2rHEzdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mYsaP5LEzdp",
        "colab_type": "text"
      },
      "source": [
        "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLT_jnZdEzdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch_training = len(train_generator)/ batch_size_training\n",
        "steps_per_epoch_validation = len(validation_generator)/ batch_size_validation\n",
        "num_epochs = 2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4NqdsuREzdy",
        "colab_type": "text"
      },
      "source": [
        "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBqnReACEzdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5ab33a8c-eed6-4e82-8712-a1622e02f947"
      },
      "source": [
        "fit_history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch_training,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=steps_per_epoch_validation,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4/3 [=======================================] - 13s 3s/step - loss: 0.7089 - accuracy: 0.6050 - val_loss: 0.6476 - val_accuracy: 0.6850\n",
            "Epoch 2/2\n",
            "4/3 [=======================================] - 3s 730ms/step - loss: 0.4301 - accuracy: 0.8100 - val_loss: 0.5127 - val_accuracy: 0.7100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlN_yDc-Ezd6",
        "colab_type": "text"
      },
      "source": [
        "Now that the model is trained, you are ready to start using it to classify images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAVlQzX4Ezd9",
        "colab_type": "text"
      },
      "source": [
        "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ForBg7GtEzd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('classifier_resnet_model.h5')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRoZRjBcEzeD",
        "colab_type": "text"
      },
      "source": [
        "<h1>----------------------------------- ResNet50 Model is trained and saved ------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGGTLRDYEzeD",
        "colab_type": "text"
      },
      "source": [
        "#### This whole portion can be skipped by importing  *load_model*  and then previously saved model can be used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhVxlfwLEzeE",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtCzGfNkEzeG",
        "colab_type": "text"
      },
      "source": [
        "Use the <code>wget</code> command to download the data for this assignment from here: https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO6XLMV1EzeH",
        "colab_type": "text"
      },
      "source": [
        "Use the following cells to download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VURr0KQaEzeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "8fffe99b-23aa-41d6-8402-f30dfbfef5af"
      },
      "source": [
        " !wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-10 21:44:44--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261483817 (249M) [application/zip]\n",
            "Saving to: ‘concrete_data_week4.zip.3’\n",
            "\n",
            "concrete_data_week4 100%[===================>] 249.37M  29.9MB/s    in 8.3s    \n",
            "\n",
            "2020-07-10 21:44:53 (30.0 MB/s) - ‘concrete_data_week4.zip.3’ saved [261483817/261483817]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvWB1r2EzeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip concrete_data_week4.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXa8HgOrEzeT",
        "colab_type": "text"
      },
      "source": [
        "After you unzip the data, you fill find the data has already been divided into a train, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnfuVcl-EzeU",
        "colab_type": "text"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "6zL8p7BuEzeU",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"item42\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff1eRJskEzeV",
        "colab_type": "text"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twMvQOdBEzeW",
        "colab_type": "text"
      },
      "source": [
        "In this part, you will design a classifier using the VGG16 pre-trained model. Just like the ResNet50 model, you can import the model <code>VGG16</code> from <code>keras.applications</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1KfS_WsEzeX",
        "colab_type": "text"
      },
      "source": [
        "You will essentially build your classifier as follows:\n",
        "1. Import libraries, modules, and packages you will need. Make sure to import the *preprocess_input* function from <code>keras.applications.vgg16</code>.\n",
        "2. Use a batch size of 100 images for both training and validation.\n",
        "3. Construct an ImageDataGenerator for the training set and another one for the validation set. VGG16 was originally trained on 224 × 224 images, so make sure to address that when defining the ImageDataGenerator instances.\n",
        "4. Create a sequential model using Keras. Add VGG16 model to it and dense layer.\n",
        "5. Compile the mode using the adam optimizer and the categorical_crossentropy loss function.\n",
        "6. Fit the model on the augmented data using the ImageDataGenerators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trCOwYrhEzeb",
        "colab_type": "text"
      },
      "source": [
        "Use the following cells to create your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgQHCUrkEzec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input as pi"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY7QUlSGEzeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes_vgg16 = 2\n",
        "image_resize_week4 = 224\n",
        "batch2_size_training = 100\n",
        "batch2_size_validation = 100"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "W24jpZACEzek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_generator_2 = ImageDataGenerator(\n",
        "    preprocessing_function=pi,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egZW4T6UEzer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9a1a04c-f8c2-426d-c453-35049744a53e"
      },
      "source": [
        "train_generator_2 = data_generator_2.flow_from_directory(\n",
        "    'concrete_data_week4/train',\n",
        "    target_size=(image_resize_week4, image_resize_week4),\n",
        "    batch_size=batch2_size_training,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 30001 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxDh6P7kEzez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3ae881f-d189-4acd-ebe5-60f93e334414"
      },
      "source": [
        "validation_generator_2 = data_generator_2.flow_from_directory(\n",
        "    'concrete_data_week4/valid',\n",
        "    target_size=(image_resize_week4, image_resize_week4),\n",
        "    batch_size=batch2_size_validation,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9501 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHOtoZVLEze5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = Sequential()\n",
        "#model = tensorflow.keras.Sequential()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNihyuSbEzfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.add(VGG16(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    ))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dPRxAzjEzfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.add(Dense(num_classes_vgg16, activation='softmax'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsItGbesEzfM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "afaddd7b-1096-46df-97b3-312aab874a1e"
      },
      "source": [
        "model_2.layers"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.training.Model at 0x7f4339f2ee48>,\n",
              " <keras.layers.core.Dense at 0x7f4330aba160>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEELdyLCEzfW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "0247ded2-6c4b-4a85-8001-61207887f543"
      },
      "source": [
        "model_2.layers[0].layers"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f4330aba2b0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4330aba518>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4330aba3c8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4330aba860>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4330aba588>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4330d81da0>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4330876860>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f43308766a0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f431b7d6d30>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f12908>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4339f17390>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f171d0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f17eb8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f1ca58>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4339f224e0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f22320>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f22fd0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f4339f26ba8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f4339f2e630>,\n",
              " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f4339f2e470>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrut7xEVEzfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.layers[0].trainable = False"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHyWpxUnEzfd",
        "colab_type": "text"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCA_cx9WEzfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "fd0b0937-4f46-4ed1-f78e-d8c344f485e9"
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 512)               14714688  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 14,715,714\n",
            "Trainable params: 1,026\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x8z-9ImEzfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6xtS8L2Ezf1",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"item43\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ivTer7PEzf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch_training_2 = len(train_generator_2)/ batch2_size_training\n",
        "steps_per_epoch_validation_2 = len(validation_generator_2)/ batch2_size_validation\n",
        "num_epochs_2 = 10"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkboCx5vEzgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "29d238a2-c6d5-4c16-816b-ceec812d016f"
      },
      "source": [
        "fit_history = model_2.fit_generator(\n",
        "    train_generator_2,\n",
        "    steps_per_epoch=steps_per_epoch_training_2,\n",
        "    epochs=num_epochs_2,\n",
        "    validation_data=validation_generator_2,\n",
        "    validation_steps=steps_per_epoch_validation_2,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4/3 [=======================================] - 11s 3s/step - loss: 0.6976 - accuracy: 0.5825 - val_loss: 0.5328 - val_accuracy: 0.6700\n",
            "Epoch 2/10\n",
            "4/3 [=======================================] - 3s 745ms/step - loss: 0.5240 - accuracy: 0.6900 - val_loss: 0.4068 - val_accuracy: 0.8000\n",
            "Epoch 3/10\n",
            "4/3 [=======================================] - 3s 742ms/step - loss: 0.3373 - accuracy: 0.8675 - val_loss: 0.3038 - val_accuracy: 0.9000\n",
            "Epoch 4/10\n",
            "4/3 [=======================================] - 3s 741ms/step - loss: 0.3201 - accuracy: 0.8825 - val_loss: 0.2008 - val_accuracy: 0.9200\n",
            "Epoch 5/10\n",
            "4/3 [=======================================] - 3s 732ms/step - loss: 0.2446 - accuracy: 0.9450 - val_loss: 0.1869 - val_accuracy: 0.9300\n",
            "Epoch 6/10\n",
            "4/3 [=======================================] - 3s 724ms/step - loss: 0.2075 - accuracy: 0.9500 - val_loss: 0.1690 - val_accuracy: 0.9600\n",
            "Epoch 7/10\n",
            "4/3 [=======================================] - 3s 714ms/step - loss: 0.1648 - accuracy: 0.9725 - val_loss: 0.1756 - val_accuracy: 0.9200\n",
            "Epoch 8/10\n",
            "4/3 [=======================================] - 3s 710ms/step - loss: 0.1392 - accuracy: 0.9700 - val_loss: 0.0875 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "4/3 [=======================================] - 3s 704ms/step - loss: 0.1348 - accuracy: 0.9625 - val_loss: 0.0820 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "4/3 [=======================================] - 3s 699ms/step - loss: 0.0986 - accuracy: 0.9850 - val_loss: 0.0896 - val_accuracy: 0.9900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htfdohz3EzgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.save('classifier_vgg16_model.h5')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmJQEgKmEzgR",
        "colab_type": "text"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn2NN3I1EzgU",
        "colab_type": "text"
      },
      "source": [
        "In this part, you will evaluate your deep learning models on a test data. For this part, you will need to do the following:\n",
        "\n",
        "1. Load your saved model that was built using the ResNet50 model. \n",
        "2. Construct an ImageDataGenerator for the test set. For this ImageDataGenerator instance, you only need to pass the directory of the test images, target size, and the **shuffle** parameter and set it to False.\n",
        "3. Use the **evaluate_generator** method to evaluate your models on the test data, by passing the above ImageDataGenerator as an argument. You can learn more about **evaluate_generator** [here](https://keras.io/models/sequential/).\n",
        "4. Print the performance of the classifier using the VGG16 pre-trained model.\n",
        "5. Print the performance of the classifier using the ResNet pre-trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FohvwiHAEzgV",
        "colab_type": "text"
      },
      "source": [
        "Use the following cells to evaluate your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrwWrZjlEzgX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aaab9352-ddd4-45e8-f1d2-906c7d7599e7"
      },
      "source": [
        "test_generator = data_generator_2.flow_from_directory(\n",
        "    'concrete_data_week4/test',\n",
        "    target_size=(image_resize_week4, image_resize_week4),\n",
        "    shuffle=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KomwMiDEzgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch_test_2 = len(test_generator)/ batch2_size_validation"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0hNlXS5Ezgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1nZGdbNEzg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ca412909-3a5b-41b9-f5bd-3c0032068d44"
      },
      "source": [
        "pretrained_model_1 = load_model('classifier_resnet_model.h5')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "  warnings.warn('Error in loading the saved optimizer '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq9jAa4IEzhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cc506e39-2daf-4652-8d92-6d165c45f6f5"
      },
      "source": [
        "pretrained_model_2 = load_model('classifier_vgg16_model.h5')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "  warnings.warn('Error in loading the saved optimizer '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f67rE3riEzhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_1 = pretrained_model_1.evaluate_generator(test_generator, steps=steps_per_epoch_test_2)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgVsyogFEzhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_2 = pretrained_model_2.evaluate_generator(test_generator, steps=steps_per_epoch_test_2)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rRwUCL6YEzhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c1930096-bfde-402b-be27-586ca4ce980c"
      },
      "source": [
        "print('Accuracy of ResNet50: {}% \\n Error: {}'.format(score_1[1], 1 - score_1[1]))        \n",
        "print('Accuracy of VGG16: {}% \\n Error: {}'.format(score_2[1], 1 - score_2[1]))        "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of ResNet50: 0.96875% \n",
            " Error: 0.03125\n",
            "Accuracy of VGG16: 1.0% \n",
            " Error: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy3I1IKJEzhh",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"item44\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP0bhvC8Ezhi",
        "colab_type": "text"
      },
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OjesKxiEzhn",
        "colab_type": "text"
      },
      "source": [
        "In this model, you will predict whether the images in the test data are images of cracked concrete or not. You will do the following:\n",
        "\n",
        "1. Use the **predict_generator** method to predict the class of the images in the test data, by passing the test data ImageDataGenerator instance defined in the previous part as an argument. You can learn more about the **predict_generator** method [here](https://keras.io/models/sequential/).\n",
        "2. Report the class predictions of the first five images in the test set. You should print something list this:\n",
        "\n",
        "<center>\n",
        "    <ul style=\"list-style-type:none\">\n",
        "        <li>Positive</li>  \n",
        "        <li>Negative</li> \n",
        "        <li>Positive</li>\n",
        "        <li>Positive</li>\n",
        "        <li>Negative</li>\n",
        "    </ul>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgFfR0pbEzho",
        "colab_type": "text"
      },
      "source": [
        "Use the following cells to make your predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8aIrQpiEzho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65s6GBmFEzh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict1 = pretrained_model_1.predict_generator(test_generator, steps=steps_per_epoch_test_2)\n",
        "predict2 = pretrained_model_2.predict_generator(test_generator, steps=steps_per_epoch_test_2)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_775QSKAEziE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84b85db5-ff81-4114-dc3b-0fc5bcc1a10c"
      },
      "source": [
        "# get the dictionary of classes\n",
        "label2index = test_generator.class_indices\n",
        "\n",
        "# obtain the list of classes\n",
        "list_of_label = list(label2index.keys())\n",
        "print(\"The list of classes: \", list_of_label)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The list of classes:  ['negative', 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPCBIR42EziS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "28d02bc4-7b03-4b84-e3e4-f7805ff44913"
      },
      "source": [
        "print(\"ResNet50 Predictions :\")\n",
        "for i in range(0,5):\n",
        "    pred_class = np.argmax(predict1[i])\n",
        "    pred_label = list_of_label[pred_class]\n",
        "    print('Prediction :{}'.format(\n",
        "        pred_label,\n",
        "        predict1[i][pred_class]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet50 Predictions :\n",
            "Prediction :negative\n",
            "Prediction :negative\n",
            "Prediction :negative\n",
            "Prediction :negative\n",
            "Prediction :negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RXTEf6Sq5Zc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "77ee2151-d825-4db1-bcb7-0a6e651d62d6"
      },
      "source": [
        "print(\"VGG16 Predictions :\")\n",
        "for i in range(0,5):\n",
        "    pred_class = np.argmax(predict2[i])\n",
        "    pred_label = list_of_label[pred_class]\n",
        "    print('Prediction :{}'.format(\n",
        "        pred_label,\n",
        "        predict2[i][pred_class]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16 Predictions :\n",
            "Prediction :negative\n",
            "Prediction :negative\n",
            "Prediction :negative\n",
            "Prediction :negative\n",
            "Prediction :negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuyOU3NeEzib",
        "colab_type": "text"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "3I8N59avEzic",
        "colab_type": "text"
      },
      "source": [
        "### Thank you for completing this lab!\n",
        "\n",
        "This notebook was created by Alex Aklson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "8ddQN4SUEzid",
        "colab_type": "text"
      },
      "source": [
        "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week4_LAB1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "iXWl5NFiEzie",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
      ]
    }
  ]
}